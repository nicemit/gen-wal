# Gen-Wal Configuration
# =====================
# For detailed documentation, please refer to: docs/CONFIGURATION.md

# -----------------------------------------------------------------------------
# 1. Profile Settings
# -----------------------------------------------------------------------------
# You can use the "provider:subtype" syntax to traverse nested configurations.
# Example: "pollinations:text" looks for config['pollinations']['text']
# -----------------------------------------------------------------------------
# Choose where to load the user profile from.
# Options: "local_file"
profile_provider: "local_file"
profile_path: "profiles/example_profile.md" # Path to your motivation profile

# -----------------------------------------------------------------------------
# 2. Quote Provider
# -----------------------------------------------------------------------------
# Quote Provider options: "llm:ollama", "pollinations:text", "csv:work_quotes"
# quote_provider: "llm:ollama"
quote_provider: "pollinations:text"

# -----------------------------------------------------------------------------
# 3. Image Provider
# -----------------------------------------------------------------------------
# Choose how to generate backgrounds.
# Options: "pollinations:image", "local_dir"
image_provider: "pollinations:image"

# Image Prompt Provider (Optional)
# Generates a dynamic visual description based on the quote + profile.
# Options: "pollinations:text", "llm:ollama", "llm:local_server". 
# If commented out, uses a static default prompt.
image_prompt_provider: "pollinations:text"

# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# 4. LLM Configuration (For 'llm' provider)
# -----------------------------------------------------------------------------
# Supports OpenAI-compatible APIs (Ollama, Llama.cpp, vLLM, OpenAI, etc.)
llm:
  # Profile 1: Ollama
  ollama:
    base_url: "http://localhost:11434/v1"
    api_key: "ollama"
    model: "llama3"

  # Profile 2: Llama.cpp
  local_server:
    base_url: "http://localhost:8080/v1"
    api_key: "sk-no-key-needed"
    model: "MaziyarPanahi/Qwen3-4B-GGUF:Q6_K" 
    request_params:
      temperature: 0.8
      max_tokens: 2000


# -----------------------------------------------------------------------------
# 5. Local Provider Configurations
# -----------------------------------------------------------------------------



# Unified Pollinations Configuration
pollinations:
  image:
    model: "flux" # Options: flux, turbo, etc.
    nologo: true # Remove watermark
    api_key: "YOUR_API_KEY_HERE" # Optional
    
  text:
    model: "openai" # Options: openai, mistral, llama
    # api_key: "..." # Optional



# -----------------------------------------------------------------------------
# 6. Output Settings
# -----------------------------------------------------------------------------
resolution:
  width: 1920
  height: 1080

# -----------------------------------------------------------------------------
# 7. Text Rendering
# -----------------------------------------------------------------------------
# text_position Options: 
#   center, top_left, top_right, bottom_left, bottom_right, 
#   top_center, bottom_center, left_center, right_center
text_position: "bottom_right"

text_padding: 100 # Distance from screen edges in pixels
font_size: 60     # Size of the quote text

# -----------------------------------------------------------------------------
# 8. Local & Custom Data Sources
# -----------------------------------------------------------------------------

# CSV Quote Provider (Access via 'quote_provider: "csv"')
csv:
  file: "my_quotes.csv"

# Local Directory Image Provider (Access via 'image_provider: "local_dir"')
local_dir:
  path: "/home/user/Pictures/Wallpapers"

